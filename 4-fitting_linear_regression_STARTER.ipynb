{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TITLE for 1 -->\n",
    "# 1.0 What is Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- INTRODUCTION for 1.1 -->\n",
    "\n",
    "A linear regression is a machine learning technique that models the relationship between explanatory variables (also called predictor variables, independent variables, or x-variables) and a continuous dependent variable (also called the target or y-variable). In the Boston housing dataset, the target variable is MEDV (median home value). It's considered continuous because there are unlimited possibilities (ranging from zero to infinity) for what value this variable could take. This is in contrast to discrete variables, which you can think of as categories. For discrete variables, values can only fall into one out of a finite number of options. You'll use linear regression to find the relationship between the predictor variables (nitric oxide concentrations, proximity to the Charles River, etc), and the median home value for towns in Boston."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 1.1 -->\n",
    "## 1.1 Linear Regression for Predictions\n",
    "\n",
    "There are two common scenarios in which linear regression may be useful. The first is making predictions or forecasting. In these situations, you fit a model to learn the relationship between some predictor variables and a target variable. If you want to know the value of the target variable in some new case, the model can make a prediction for you. One example might be forecasting the profits of a business in the next quarter based on data you've collected this quarter. For the Boston housing data, you'll be able to predict the median home value for new towns (as long as you collect data on nitric oxide levels, number of rooms per home, etc). Notice that you can either predict future values, or simply values for cases you haven't seen before.\n",
    "\n",
    "Think of a few other examples of when you might use linear regression to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 1.1\n",
    "\n",
    "# Linear regression could be used to predict..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT for 1.1-->\n",
    "Double click for hint.\n",
    "<!-- Think about sets of related variables. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 1.2 -->\n",
    "## 1.2 A Quick Math Interlude\n",
    "\n",
    "Before you learn about the second use for linear regressions, let's take a few minutes to talk math. Given that the word \"linear\" is in the name, you might have guessed that linear regressions have to do with lines.\n",
    "\n",
    "Remember the equation for a line? $$y = mx + b $$\n",
    "\n",
    "Here, y represents the target or dependent variable, m is the slope, x is a predictor variable, and b is the y-intercept.\n",
    "\n",
    "You might also see something like this: \n",
    "\n",
    "$$y = w_1x_1 + w_0$$\n",
    "\n",
    "Or like this: \n",
    "\n",
    "$$y = \\beta_1x_1 + \\beta_0$$\n",
    "\n",
    "In a case where you have multiple predictor variables (like in the Boston housing dataset), the equation might look more like this, with one value of x and one weight for every variable in the dataset:\n",
    "\n",
    "$$y = w_3x_3 + w_2x_2 + w_1x_1 + x_0$$\n",
    "\n",
    "\n",
    "All of these are telling you the same thing: that you can calculate the value of the target variable (y) by multiplying predictor variables (x) by weights (m, w, or $\\beta$).\n",
    "\n",
    "When you use your computer to fit a linear regression, it's basically calculating the values of the weights (also called regression coefficients) to make the best guess for y.\n",
    "\n",
    "If you'd like to learn more about these concepts, check out this [video](https://www.youtube.com/watch?v=KsVBBJRb9TE) or this [article](http://onlinestatbook.com/2/regression/intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 1.3 -->\n",
    "## 1.3 Linear Regression for Understanding Relationships\n",
    "\n",
    "Now, back to explaining why linear regression can be useful. Looking at the values your computer will calculate for the weights (m, w, or $\\beta$) can tell you which variables are the best predictors of the target variable. Variables with the highest or lowest coefficients have the strongest relationship with the target variable. As these variables increase or decrease, so does the target variable.\n",
    "\n",
    "If a linear regression calculates a very positive coefficient for a particular predictor variable, this means that there is a positive relationship between it and the target. As the predictor increases, so does the target. (Imagine the relationship between working more hours and having a higher paycheck. As you work more at your hourly wage job, you earn more money.) \n",
    "\n",
    "If the regression reveals a very negative coefficient for a predictor variable, it means there is an inverse relationship between it and the target. As the predictor decreases, the target increases. (For example, there is an inverse relationship between the number of coats in your closet when you throw a party and how much empty space there is. As you add more, the amount of space goes down.) \n",
    "\n",
    "Finally, if the coefficient is close to zero, the predictor has a negligible effect on the target. (Such as the relationship between the temperature outside and how tall you are. The temperature can move up or down, but your height won't change.)\n",
    "\n",
    "Think of other relationships between variables that might yield a positive, negative or neutral regression coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 1.3\n",
    "# Positive regression coefficient:\n",
    "# Negative regression coefficient:\n",
    "# Neutral regression coefficient:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- CONCLUSION for 1.0 -->\n",
    "In this step, you learned about your first machine learning technique: linear regression. You know that it can be used to make predictions or to understand the relationships between variables, and saw a few examples. Next, you'll use Scikit-Learn to fit a linear regression on the Boston housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TITLE for 2 -->\n",
    "# 2.0 Fitting a Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-run\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_boston()\n",
    "data = pd.DataFrame(dataset.data, columns = dataset.feature_names)\n",
    "data['MEDV'] = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- INTRODUCTION for 2.1 -->\n",
    "Now that you know what a linear regression is, and what it can be used for, it's time to fit one. In this lesson, you'll become familiar with the Scikit-Learn library, which contains tools for machine learning in Python, including (but certainly not limited to) linear regression. Using the Boston housing data, you'll learn to fit a linear regression that can predict a town's median home value based on the other variables (proportion of homes built before 1940, nitric oxide concentration, accessibility to highways, etc).\n",
    "\n",
    "When you're modeling data in Scikit-Learn, there are three basic steps to remember: **fit** the model, **predict** the target variable, and **evaluate** the model's fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 2.1 -->\n",
    "## 2.1 Import LinearRegression\n",
    "\n",
    "Throughout this step, you'll be creating a LinearRegression object and calling different methods on it.\n",
    "\n",
    "Import the LinearRegression class from Scikit-Learn.\n",
    "Create a LinearRegression object and assign it to the variable 'model'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 2.1\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT for 2.1 -->\n",
    "Double click for hint.\n",
    "<!-- Create an instance of a linear regression model by declaring `LinearRegression()`. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 2.2 -->\n",
    "## 2.2 Assign Predictor and Target Values\n",
    "\n",
    "To fit a model in Scikit-Learn, the target and predictor variables need to be in separate arrays. It's conventional to assign all values of predictor variables to an array called X, and the column of the target variable to another array called y.\n",
    "\n",
    "Assign the predictor variables to X and the target variable to y. Check their dimensions to be sure they have the same number of observations (506 different towns), and that X has 13 columns (one for each predictor variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 2.2\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT -->\n",
    "Double click for hint.\n",
    "<!-- You can find the predictor variables in the `.data` attribute of the original Boston dataset import from sklearn, and the target variable in the `.target` attribute. Use `.shape` to see the dimensions of each array. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 2.3 -->\n",
    "## 2.3 Fit the Model\n",
    "\n",
    "One of the things that makes Scikit-Learn easy to use is that many different models have the same methods, so once you've learned how to implement one modeling technique, the process for implementing other models others will look very similar. The first method you'll learn is `.fit()`.\n",
    "\n",
    "The `.fit()` method takes arrays of predictor variables and target variables as arguments, and trains the model to learn the relationship between them. \n",
    "\n",
    "Fit the model on the X and y arrays you just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 2.3\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT for 2.3-->\n",
    "Double click for hint.\n",
    "<!-- Add X and y inside `.fit()`. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 2.4 -->\n",
    "## 2.4 Predict Values of y\n",
    "\n",
    "Now, the model has been trained on the relationship between the predictor variables and the target variable. Under the hood, Scikit-Learn has calculated a coefficient for every predictor variable. Given a new observation of a town for which you've measured proportion of old homes, nitric oxide levels, etc, the model can predict its median home value by calculating the linear combination of all values using these coefficients.\n",
    "\n",
    "Each Scikit-Learn model type has a `.predict()` method that takes in an array of predictor variables and outputs the model's \"guess\" for the target variables. \n",
    "\n",
    "Use `.predict()` to create an array of predicted median home values for every observation in X and assign it to the variable \"predictions\". Check the length of X and the length of the \"predictions\" array. (They should be the same, since there's one prediction for every town in X.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 2.4\n",
    "predictions ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT -->\n",
    "Double click for hint.\n",
    "<!-- Remember that you can check the length of an array using `len()`. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- CONCLUSION for 2.0 -->\n",
    "In this step, you've created a linear regression model in Scikit-Learn, trained it on the predictor and target variables using the `.fit()` method, and used it to make its own predictions for median home value with the `.predict()` method. The next question is: how did it perform? Next, you'll learn how to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TITLE for 3 -->\n",
    "# 3.0 Evaluating the Model Graphically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- INTRODUCTION for 2 -->\n",
    "Now that you've trained the model and had it make some predictions, it's time to evaluate its performance. How accurate are its guesses for median home value compared to the actual measured values of MEDV in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 3.1 -->\n",
    "## 3.1 Plotting Measured Values vs Predictions\n",
    "\n",
    "One way to visually assess the model's performance is to plot the recorded median home values from the dataset against the predicted values that the model generated. If the model predicted perfectly for every town, the points would fall on a straight diagonal line because the predicted value would equal the measured value. The less accurate the guesses are, the more the dots will diverge from the line.\n",
    "\n",
    "Make a scatterplot with the observed values of MEDV on the x-axis and the model's predictions on the y-axis. Don't forget to include axis labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 3.1\n",
    "# add code for scatter plot here\n",
    "plt.plot([0,60],[0,60], color = 'red')\n",
    "# add code here\n",
    "plt.ylim(0,60)\n",
    "plt.xlim(0,60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT -->\n",
    "Double click for hint.\n",
    "\n",
    "<!-- Remember that you stored the MEDV values under the variable y, and that since these go on the x-axis, y will be the first argument in the .scatter() function. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 3.2 -->\n",
    "## 3.2 Investigating Coefficients\n",
    "\n",
    "You've just plotted the actual median home values against the model's predictions. Were the points clustered along a line or diffusely spread? Does it look like the model's predictions were close to the actual values?\n",
    "\n",
    "When the model calculated its predictions for median home value, it assigned various weights to each of the target variables. This means that each of the factors like AGE, RAD, or NOX have a differing level of importance in determining the final outcome. Think back to some of the plots you made during exploratory data analysis. Some of the variables, like RM, seemed to have a stronger correlation with MEDV than others, like RAD. Having a stronger correlation makes these better predictors, and the model will calculate higher coefficients for them.\n",
    "\n",
    "Each Scikit-Learn estimator object stores the coefficients it calculates in the .coef\\_ attribute. To see which variables had the highest coefficient (and therefore the strongest weight in the guess for median home value), you can make a dataframe that lists variables and coefficients and sort it from highest to lowest.\n",
    "\n",
    "Assign the model.coef\\_ to the 'coefficient' column of the coefficient_df dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 3.2\n",
    "coefficient_df = pd.DataFrame(columns = ['variable', 'coefficient'])\n",
    "X = pd.DataFrame(X)\n",
    "coefficient_df['variable'] = X.columns\n",
    "# add code here\n",
    "coefficient_df.sort_values('coefficient', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT for 3.2-->\n",
    "Double click for hint.\n",
    "<!-- You can assign the value of the coefficient column like this: `coefficient_df['coefficient'] =` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 3.3 -->\n",
    "## 3.3 Plotting Coefficients\n",
    "\n",
    "You've probably noticed that some of the coefficients for the predictor variables are positive and others are negative. Remember, this represents the direction of the relationship between each variable and the predictor: a very negative coefficient means that the two variables are inversely related. For instance, as NOX increases, MEDV goes down. A positive coefficient means that the variables are positively related: as RM increases, so does MEDV. The further a coefficient is from zero in either direction, the stronger the relationship. Coefficients close to zero have little effect on the median home value.\n",
    "\n",
    "To visualize these trends, you can plot the coefficients on a bar graph. The longest bars in either direction belong to the variables that had the most influence over the model's predictions.\n",
    "\n",
    "Assign the values in the `coefficient` column of the dataframe to `bar_heights`.\n",
    "Assign the values in the `variable` column of the dataframe to `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 3.3\n",
    "# sort the coefficient dataframe from highest to lowest\n",
    "coefficient_df = coefficient_df.sort_values('coefficient', ascending = False)\n",
    "\n",
    "# assign bar heights, positions, and labels\n",
    "bar_heights = \n",
    "bar_positions = np.arange(0.5, 13.5, 1)\n",
    "labels = \n",
    "\n",
    "# make plot\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.bar(left = bar_positions, height = bar_heights)\n",
    "plt.xticks(np.arange(0.75, 13.75, 1), labels, rotation = 70)\n",
    "plt.ylabel('Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT -->\n",
    "Double click for hint.\n",
    "<!-- Remember that you can refer to a column in a dataframe by slicing. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- CONCLUSION for 3.0 -->\n",
    "In this step, you evaluated the performance of the model visually by comparing the predictions it made for home value to the recorded values from the dataset, and plotting the coefficients for each of the predictor variables. From these plots, it appears that the model fits reasonably well, and the most important factors in predicting home value are the nitric oxide concentration and the number of rooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- INTRODUCTION for 4 -->\n",
    "# 4.0 Using Evaluation Metrics\n",
    "\n",
    "In this lesson, you'll learn how to evaluate a linear regression using the R-squared value, mean absolute error, and root mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 3.1 -->\n",
    "## 4.1 Introducing the R-squared value\n",
    "\n",
    "For a more exact interpretation of how well the model fits the data, you can look at the R-squared value (also known as the coefficient of determination). Although there were 13 predictor variables in this multiple regression, imagine training the model on a single variable. You could plot that single variable on the x-axis and the target variable on the y-axis. If you drew a line through the resulting scatter of points to minimize the distance between each point and the line, you'd have draw the line of best fit. (This is basically what a linear regression does when it make predictions for the target variable.) You can quantify how well the line fits the points using the R-squared value. When there is a perfect fit (not a single point off the line), the R-squared value is equal to one.\n",
    "\n",
    "Look at the example plot with a simulated target and predictor variable. Each data point falls exactly on the the line, so the R-squared for the imaginary model that generated this data would be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 4.1\n",
    "# Plotting the predicted values for a model where R-squared = 1\n",
    "plt.scatter([0, 0.5, 1, 1.5, 2, 2.5, 3], [0, 0.5, 1, 1.5, 2, 2.5,3], label = 'actual values', color = 'red')\n",
    "plt.plot([0,3], [0,3], label = 'predicted values')\n",
    "plt.xlabel('Predictor Variable')\n",
    "plt.ylabel('Target Variable')\n",
    "plt.legend(loc = 'best')\n",
    "plt.xlim(0,3)\n",
    "plt.ylim(0,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT for 4.1-->\n",
    "Double click for hint.\n",
    "<!-- Just run the cell. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 4.2 -->\n",
    "## 4.2 Calculating R-squared\n",
    "\n",
    "The example you just saw was for a single linear regression where there was only one predictor and one target variable. Think about doing this for our 13-factor multiple linear regression... You'd have to plot each variable on its own axis in 13-dimensional space. This doesn't visualize well, but the bottom line is the same - the closer R-squared is to one, the better the model's fit.\n",
    "\n",
    "The `.score()` method takes the arrays of predictor and target variables as arguments and returns the R-squared value of the model.\n",
    "\n",
    "Assign the R-squared value of the model to the variable `Rsq` and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 4.2\n",
    "Rsq = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT for 4.2-->\n",
    "Double click for hint.\n",
    "<!-- The function operates on the model you've built, so call `model.score()` and input X and y. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 4.3 -->\n",
    "## 4.3 Mean Absolute Error\n",
    "\n",
    "Another value that you can use to assess your model's performance is the mean absolute error, or MAE. This metric is an average of the distance between each predicted value and the corresponding actual value. It can be useful when you want to know how far off your model was on average, in meaningful units.\n",
    "\n",
    "Since the target variable, median home value, was measured in thousands of dollars, the mean absolute error will also be in thousands of dollars.\n",
    "\n",
    "You can calculate MAE using the `mean_absolute_error()` function. It takes the actual target variables and the predicted target variables as arguments.\n",
    "\n",
    "Calculate the MAE of y and the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 4.3\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT for 4.3-->\n",
    "Double click for hint.\n",
    "\n",
    "<!-- Input `y` and `predictions` as arguments to the `mean_absolute_error()` function. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TASK 4.4 -->\n",
    "## 4.4 Root Mean Squared Error\n",
    "\n",
    "You just saw that the mean absolute error for the model was 3.27. Since the units for home value were thousands of dollars, this means, on average, the model's prediction for median home value were off by \\$3,270.\n",
    "\n",
    "When it comes to deciding between evaluating a model using MAE, or another metric, root mean squared error (RMSE), you must consider the effect of outliers. An outlier is a value very far from the mean and median of the dataset. Given that our model was trained on median home values ranging from about \\$3,000 to \\$55,000, we can be confident that it would make accurate predictions within this range. However, what if we asked the model to predict the value for a town whose median home value was \\$100,000. This is far outside the range the model trained on, so it's likely going to guess way wrong.\n",
    "\n",
    "In this situation, when a model guesses horribly wrongly for a few outliers, the MAE will change, but not a lot. It doesn't penalize the model too much for being very wrong, as long as it's only a few times.\n",
    "\n",
    "If you prefer that your model be sensitive to outliers, but don't necessarily care about measuring how wrong outlier predictions were in units, RMSE is the appropriate metric to use.\n",
    "\n",
    "Scikit-Learn has a `mean_squared_error(`) function, and you can calculate RMSE by simply taking the square root of the mean squared error.\n",
    "\n",
    "Calculate the RMSE for y and the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STARTER CODE for 4.4\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rmse = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- HINT for 4.4-->\n",
    "Double click for hint.\n",
    "\n",
    "<!-- The `sqrt()` function returns the square root of a value. Apply this to the output of the `mean_squared_error()` function. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- CONCLUSION for 4.0 -->\n",
    "In this lesson, you've accomplished a lot: you created arrays of target and predictor variables, fit a linear regression, learned which variables had the most predictive power, and then evaluated model fit in 4 different ways. Now that you know how to do this for linear regressions using Scikit-Learn, you can easily transfer these same steps to a logistic regression or some other type of model. Just remember the basic steps: **fit**, **predict**, and **evaluate**."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
